{"organizations": [], "uuid": "e504f2663b1f16411fc0628e24517e3f4159f732", "thread": {"social": {"gplus": {"shares": 1}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 85}, "facebook": {"likes": 184, "shares": 184, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.ft.com", "main_image": "http://prod-upp-image-read.ft.com/24461c60-f257-11e6-95ee-f14e55513608", "site_section": "", "section_title": "", "url": "https://www.ft.com/content/8e228692-f251-11e6-8758-6876151821a6", "country": "US", "domain_rank": 1571, "title": "Frankenstein fears hang over AI", "performance_score": 1, "site": "ft.com", "participants_count": 0, "title_full": "Frankenstein fears hang over AI", "spam_score": 0.754, "site_type": "news", "published": "2017-02-16T12:14:00.000+02:00", "replies_count": 0, "uuid": "e504f2663b1f16411fc0628e24517e3f4159f732"}, "author": "", "url": "https://www.ft.com/content/8e228692-f251-11e6-8758-6876151821a6", "ord_in_thread": 0, "title": "Frankenstein fears hang over AI", "locations": [], "entities": {"persons": [{"name": "frankenstein", "sentiment": "negative"}, {"name": "satya nadella", "sentiment": "none"}, {"name": "horvitz", "sentiment": "none"}, {"name": "donald trump", "sentiment": "none"}, {"name": "stuart russell", "sentiment": "none"}, {"name": "nadella", "sentiment": "none"}, {"name": "adam jezard sample", "sentiment": "none"}, {"name": "mckinsey", "sentiment": "none"}, {"name": "russell", "sentiment": "none"}, {"name": "bill gates", "sentiment": "none"}, {"name": "joi ito", "sentiment": "none"}, {"name": "vivek wadhwa", "sentiment": "none"}, {"name": "eric horvitz", "sentiment": "none"}, {"name": "elon musk", "sentiment": "none"}], "locations": [{"name": "us", "sentiment": "none"}, {"name": "eu", "sentiment": "none"}, {"name": "uk", "sentiment": "none"}, {"name": "davos", "sentiment": "none"}], "organizations": [{"name": "ibm", "sentiment": "none"}, {"name": "world economic forum", "sentiment": "none"}, {"name": "harvard", "sentiment": "none"}, {"name": "benefit people and society", "sentiment": "none"}, {"name": "media lab", "sentiment": "none"}, {"name": "massachusetts institute of technology", "sentiment": "none"}, {"name": "microsoft", "sentiment": "none"}, {"name": "us senate", "sentiment": "none"}, {"name": "university of california, berkeley", "sentiment": "none"}, {"name": "mit", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "tesla motors", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Frankenstein fears hang over AI Artificially intelligent systems must not replicate human bias Read next by: Richard Waters \nThe technology industry is facing up to the world-shaking ramifications of artificial intelligence. There is now a recognition that AI will disrupt how societies operate, from education and employment to how data will be collected about people. \nMachine learning, a form of advanced pattern recognition that enables machines to make judgments by analysing large volumes of data, could greatly supplement human thought. But such soaring capabilities have stirred almost Frankenstein-like fears about whether developers can control their creations. \nFailures of autonomous systems — like the death last year of a US motorist in a partially self-driving car from Tesla Motors — have led to a focus on safety, says Stuart Russell, a professor of computer science and AI expert at the University of California, Berkeley. “That kind of event can set back the industry a long way, so there is a very straightforward economic self-interest here,” he says. \nAlongside immigration and globalisation, fears of AI-driven automation are fuelling public anxiety about inequality and job security. The election of Donald Trump as US president and the UK’s vote to leave the EU were partly driven by such concerns. While some politicians claim protectionist policies will help workers, many industry experts say most jobs losses are caused by technological change, largely automation. \nGlobal elites — those with high income and educational levels, who live in capital cities — are considerably more enthusiastic about innovation than the general population, the FT/Qualcomm Essential Future survey found. This gap, unless addressed, will continue to cause political friction. \nVivek Wadhwa, a US-based entrepreneur and academic who writes about ethics and technology, thinks the new wave of automation has geopolitical implications: “Tech companies must accept responsibility for what they’re creating and work with users and policymakers to mitigate the risks and negative impacts. They must have their people spend as much time thinking about what could go wrong as they do hyping products. \nThe industry is bracing itself for a backlash. Advances in AI and robotics have brought automation to areas of white-collar work, such as legal paperwork and analysing financial data. Some 45 per cent of US employees’ work time is spent on tasks that could be automated with existing technologies, a study by McKinsey says. \nIndustry and academic initiatives have been set up to ensure AI works to help people. These include the Partnership on AI to Benefit People and Society , established by companies including IBM, and a $27m effort involving Harvard and the Massachusetts Institute of Technology. Groups like Open AI , backed by Elon Musk and Google, have made progress, says Prof Russell: “We’ve seen papers...that address the technical problem of safety.” \nThere are echoes of past efforts to deal with the complications of a new technology. Satya Nadella, chief executive of Microsoft, compares it to 15 years ago when Bill Gates rallied his company’s developers to combat computer malware. His “trustworthy computing” initiative was a watershed moment. In an interview with the FT , Mr Nadella said he hoped to do something similar to ensure AI works to benefit humans. \nAI presents some thorny problems, however. Machine learning systems derive insights from large amounts of data. Eric Horvitz, a Microsoft executive, told a US Senate hearing late last year that these data sets may themselves be skewed. “Many of our data sets have been collected...with assumptions we may not deeply understand, and we don’t want our machine-learned applications...to be amplifying cultural biases,” he said. \nLast year, an investigation by news organisation ProPublica found that an algorithm used by the US justice system to determine whether criminal defendants were likely to reoffend, had a racial bias. Black defendants with a low risk of reoffending were more likely than white ones to be labelled as high risk. \nGreater transparency is one way forward, for example making it clear what information AI systems have used. But the “thought processes” of deep learning systems are not easy to audit.Mr Horvitz says such systems are hard for humans to understand. “We need to understand how to justify [their] decisions and how the thinking is done.” \nAs AI comes to influence more government and business decisions, the ramifications will be widespread. “How do we make sure the machines we ‘train’ don’t perpetuate and amplify the same human biases that plague society?” asks Joi Ito, director of MIT’s Media Lab. \nExecutives like Mr Nadella believe a mixture of government oversight — including, by implication, the regulation of algorithms — and industry action will be the answer. He plans to create an ethics board at Microsoft to deal with any difficult questions thrown up by AI. \nHe says: “I want...an ethics board that says, ‘If we are going to use AI in the context of anything that is doing prediction, that can actually have societal impact...that it doesn’t come with some bias that’s built in.’” \nMaking sure AI systems benefit humans without unintended consequences is difficult. Human society is incapable of defining what it wants, says Prof Russell, so programming machines to maximise the happiness of the greatest number of people is problematic. \nThis is AI’s so-called “control problem”: the risk that smart machines will single-mindedly pursue arbitrary goals even when they are undesirable. “The machine has to allow for uncertainty about what it is the human really wants,” says Prof Russell. \nEthics committees will not resolve concerns about AI taking jobs, however. Fears of a backlash were apparent at this year’s World Economic Forum in Davos as executives agonised over how to present AI. The common response was to say machines will make many jobs more fulfilling though other jobs could be replaced. \nThe profits from productivity gains for tech companies and their customers could be huge. How those should be distributed will become part of the AI debate. “Whenever someone cuts cost, that means, hopefully, a surplus is being created,” says Mr Nadella. “You can always tax surplus — you can always make sure that surplus gets distributed differently.” \nAdditional reporting by Adam Jezard Sample the FT’s top stories for a week You select the topic, we deliver the news. Select topic Invalid email Sign up By signing up you confirm that you have read and agree to the terms and conditions , cookie policy and privacy policy . Copyright The Financial Times Limited 2017. All rights reserved. You may share using our article tools. Please don't cut articles from FT.com and redistribute by email or post to the web. ", "external_links": [], "published": "2017-02-16T12:14:00.000+02:00", "crawled": "2017-02-16T07:15:14.707+02:00", "highlightTitle": ""}