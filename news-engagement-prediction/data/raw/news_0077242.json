{"organizations": [], "uuid": "afadf0f6ff996d0f1c42187120a5efb9ff187fce", "thread": {"social": {"gplus": {"shares": 1}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 230, "shares": 230, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.cbc.ca", "main_image": "https://i.cbc.ca/1.3974587.1486670434!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_1180/social-media.jpg", "site_section": "http://rss.cbc.ca/lineup/topstories.xml", "section_title": "CBC | Top Stories News", "url": "http://www.cbc.ca/news/opinion/online-toxicity-1.4001767?cmp=rss", "country": "CA", "domain_rank": 1123, "title": "Online toxicity might just be an issue of bad design", "performance_score": 2, "site": "cbc.ca", "participants_count": 1, "title_full": "Online toxicity might just be an issue of bad design", "spam_score": 0.0, "site_type": "news", "published": "2017-02-28T12:00:00.000+02:00", "replies_count": 0, "uuid": "afadf0f6ff996d0f1c42187120a5efb9ff187fce"}, "author": "cbc", "url": "http://www.cbc.ca/news/opinion/online-toxicity-1.4001767?cmp=rss", "ord_in_thread": 0, "title": "Online toxicity might just be an issue of bad design", "locations": [], "entities": {"persons": [{"name": "jigsaw", "sentiment": "none"}, {"name": "jeffrey lin", "sentiment": "none"}], "locations": [{"name": "jigsaw", "sentiment": "none"}], "organizations": [{"name": "npr", "sentiment": "none"}, {"name": "toronto star", "sentiment": "none"}, {"name": "reuters", "sentiment": "none"}, {"name": "tribunal", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "cbc", "sentiment": "none"}, {"name": "wikipedia", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "The misanthrope's view of the internet is that it's a hotbed for hate speech and angry trolling, and that it will forever be so. \nThat's because, according to this view, the online world is basically lawless frontier where we're free from the structure, confines and civility of real life. Another perspective is that the internet simply acts as a massive floodlight, exposing the ugliest parts of human nature. \nBut new approaches to taming trolls show that the current state of online toxicity may just be an issue of bad design. Companies such as Google and Riot Games — the makers of the massive multiplayer game League of Legends — are implementing new strategies to tackle poisonous speech, and these solutions might also prove successful in taming trolls on news sites and other online communities. \nThe old tactics The existing strategy for dealing with toxic speech has, by and large, been to raise a white flag: news outlets such as the Toronto Star , NPR, Reuters, Popular Science, The Telegraph and Recode have all closed down their comment sections. Others have eliminated the anonymity element, believing that by forcing commenters to use their real names, it will inspire more accountability.\nThese approaches are flawed, however. The internet is all about engaging with your audience, something that becomes much more difficult when comments are prohibited. What's more, anonymity is one of the most powerful assets of the internet: it allows individuals to explore different aspects of their identities and express their beliefs without fear. We should be able to rein in the bad without having to forfeit the good.\nBeyond that, a number of studies show that anonymity might not be driving online toxicity after all. Rather, it could very well be the lack of repercussions and real-life consequences — coupled with anonymity — that fuel nasty behaviour online. Indeed, anonymity might set the foundation for aggression, but the lack of consequences is arguably what keeps the harassment going.\nThe new tools Jeffrey Lin, a designer at Riot Games, is trying to remedy that with a tool called \"The Tribunal.\" Using The Tribunal, League of Legends players report behaviour they find unacceptable, and other community members then vote on whether they believe the behaviours permissible or not. After initially implementing the tool, Riot Games incorporated artificial intelligence to make the whole process more efficient. Humans still identify which behaviour is and isn't acceptable, but the machine learning system delivers swift, customized consequences and penalties, such as chat restrictions and temporary bans from the game.\nAnyone can be an internet troll if the situation is right \nAdvice from a 10-year-old on dealing with online trolls \nNow Google is trying a similar approach. The company's tech incubator, Jigsaw, along with its C ounter Abuse Technology team, recently launched Perspective, a public API that uses artificial intelligence to automatically flag toxic online speech. By comparing new comments with a large data set comprised of archived comments, previously flagged as toxic, from sources such as Wikipedia or online news comment sections, Jigsaw believes it can positively identify hateful speech. As a result, a user's commenting privileges may be revoked, or else, he or she might be subject to \"shadowbanning,\" whereby comments appear invisible to other members of the community.\nBoth of these models suggest that maybe the current of online toxicity isn't inevitable, or irreversible. Instead, perhaps it is just an issue of bad design, or as is often the case, of no design at all. When people step out of line, there need to be consequences, which is where good design strategies and machine learning can help. Online discourse can get better; communities just need the tools to help make it happen. \nThis column is part of CBC's  Opinion section.  For more information about this section, please read this  editor's blog  and  our FAQ .", "external_links": [], "published": "2017-02-28T12:00:00.000+02:00", "crawled": "2017-02-28T12:41:33.519+02:00", "highlightTitle": ""}