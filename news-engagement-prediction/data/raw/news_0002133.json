{"organizations": [], "uuid": "edf928cae72cfb09ef7104a57ecba05735c17cd9", "thread": {"social": {"gplus": {"shares": 3}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 26}, "facebook": {"likes": 421, "shares": 421, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.bleepingcomputer.com", "main_image": "https://www.bleepstatic.com/content/posts/2017/02/01/GitLab.png", "site_section": "http://www.bleepingcomputer.com/feed/", "section_title": "Latest news and stories from BleepingComputer.com", "url": "https://www.bleepingcomputer.com/news/hardware/gitlab-goes-down-after-employee-deletes-the-wrong-folder/", "country": "US", "domain_rank": 6585, "title": "GitLab Goes Down After Employee Deletes the Wrong Folder", "performance_score": 4, "site": "bleepingcomputer.com", "participants_count": 1, "title_full": "GitLab Goes Down After Employee Deletes the Wrong Folder", "spam_score": 0.0, "site_type": "news", "published": "2017-02-01T17:06:00.000+02:00", "replies_count": 0, "uuid": "edf928cae72cfb09ef7104a57ecba05735c17cd9"}, "author": "Catalin Cimpanu", "url": "https://www.bleepingcomputer.com/news/hardware/gitlab-goes-down-after-employee-deletes-the-wrong-folder/", "ord_in_thread": 0, "title": "GitLab Goes Down After Employee Deletes the Wrong Folder", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": [], "text": "GitLab Goes Down After Employee Deletes the Wrong Folder GitLab Goes Down After Employee Deletes the Wrong Folder By 1 \nGitLab.com, a web service for hosting and syncing source code, similar to GitHub, has gone down last night at around 18:00 ET, January 31, and after 11 hours, at the time of publishing, the website is still down. \n\"We accidentally deleted production data and might have to restore from backup,\" the GitLab team tweeted an hour after the incident started. Tired admin deleted the wrong folder \nAccording to tech news site The Register , a tired GitLab admin working late in the Netherlands might have accidentally deleted the wrong folder in a planned maintenance operation. The company never confirmed this exact scenario, but admitted that someone deleted something they shouldn't have. \nIt's believed that GitLab lost 295.5 GB of customer data during this snafu. The company immediately started recovery operations from backups. \n\"The incident affected the database (including issues and merge requests) but not the git repo's (repositories and wikis),\" GitLab later tweeted . \"Data transfer has been slow.\" Recovery operations didn't go as planned, are very slow \nIn a Google Docs file , GitLab staff had been using to keep track of their operations, the company detailed a grim scenario. \n\"So in other words, out of 5 backup/replication techniques deployed none are working reliably or set up in the first place,\" the document reads. \nGitLab staff also detail some of the other problems they've encountered trying to recover the lost data from backups. LVM snapshots are by default only taken once every 24 hours. YP happened to run one manually about 6 hours prior to the outage Regular backups seem to also only be taken once per 24 hours, though YP has not yet been able to figure out where they are stored. According to JN these don’t appear to be working, producing files only a few bytes in size. SH: It looks like pg_dump may be failing because PostgreSQL 9.2 binaries are being run instead of 9.6 binaries. This happens because omnibus only uses Pg 9.6 if data/PG_VERSION is set to 9.6, but on workers this file does not exist. As a result it defaults to 9.2, failing silently. No SQL dumps were made as a result. Fog gem may have cleaned out older backups. Disk snapshots in Azure are enabled for the NFS server, but not for the DB servers. The synchronisation process removes webhooks once it has synchronised data to staging. Unless we can pull these from a regular backup from the past 24 hours they will be lost The replication procedure is super fragile, prone to error, relies on a handful of random shell scripts, and is badly documented SH: We learned later the staging DB refresh works by taking a snapshot of the gitlab_replicator directory, prunes the replication configuration, and starts up a separate PostgreSQL server. Our backups to S3 apparently don’t work either: the bucket is empty We don’t have solid alerting/paging for when backups fails, we are seeing this in the dev host too now. \nIn a tweet posted two hours before this article and 8 hours after starting recovery operations, GitLab said it was only at 42% in the database recovery operation. \nAt this point, the GitLab downtime looks to extend well into the following day, February 1. \nUPDATE: The GitLab outage has been fixed. A GitLab spokesperson has provided the following statement regarding the events that have transpired in the last day. On Tuesday, GitLab experienced an outage for one of its products, the online service GitLab.com. This outage did not affect our Enterprise customers or the wide majority of our users. As part of our ongoing recovery efforts, we are actively investigating a potential data loss. If confirmed, this data loss would affect less than 1% of our user base, and specifically peripheral metadata that was written during a 6-hour window. We have been working around the clock to resume service on the affected product, and set up long-term measures to prevent this from happening again. We will continue to keep our community updated through Twitter, our blog and other channels.", "external_links": [], "published": "2017-02-01T17:06:00.000+02:00", "crawled": "2017-02-02T00:13:09.806+02:00", "highlightTitle": ""}