{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Engagement Prediction\n",
    "\n",
    "Create a system that predicts the engagement of a given news article based on its content (title and/or article text):\n",
    "* Data from https://webhose.io/free-datasets/popular-news-articles/\n",
    "* word2vec-CNN-BiLSTM model\n",
    "* Trained on Triton supercomputer GPUs\n",
    "* Created in Jupyter Notebook using Pytorch\n",
    "* Deliverable as requestable API on AWS\n",
    "* Possibly create a dashboard for user to test and rank different word choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pytz\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import preprocess as pp\n",
    "import graph as gp\n",
    "import hparams as hp\n",
    "import model as ml\n",
    "import train as tr\n",
    "import postprocess as pop\n",
    "import analysis as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_choice: cnn-bilstm\n",
    "ml_choice = 'cnn-bilstm'\n",
    "\n",
    "# dataset: webhose-popular\n",
    "dataset = 'webhose-popular'\n",
    "\n",
    "update_dict = {}\n",
    "update_dict['ml_choice'] = ml_choice\n",
    "update_dict['dataset'] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pp.read_data(dataset)\n",
    "data_all = pp.remove_keys(data_all)\n",
    "data_all = pp.cal_all_engagements(data_all)\n",
    "\n",
    "titles = pp.get_titles(data_all)\n",
    "data_all_ordered = pp.order_keys(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Here are The Funniest Reaction Memes To Beyonce's Pregnancy | Angie Martinez | Power 105.1 FM\",\n",
      "    \"sanitized_title\": \"here are the funniest reaction memes to beyonces pregnancy\",\n",
      "    \"text\": \"Here are The Funniest Reaction Memes To Beyonce's Pregnancy posted by Gabriel Pabon - \\nThe internet wastes no time when reacting to big news especially when it comes to Beyonce . \\nCheck out the funniest memes from today's announcement below. A photo posted by HONEY GERMAN (@honeygerman) on Feb 1, 2017 at 1:03pm PST A photo posted by HONEY GERMAN (@honeygerman) on Feb 1, 2017 at 12:57pm PST A photo posted by HONEY GERMAN (@honeygerman) on Feb 1, 2017 at 11:40am PST A photo posted by SSquared Podcast/Radio Show (@teamssquared) on Feb 1, 2017 at 2:07pm PST A photo posted by D-Roc \\u264f\\ufe0f (@inked_scorpio) on Feb 1, 2017 at 2:07pm PST A photo posted by At Random With Yeasha (@random_yeasha) on Feb 1, 2017 at 2:05pm PST Left or Right? \\uf62d #beyonce pic.twitter.com/PPhB9QmUAy via memes A photo posted by Mina Gerges \\uf48e (@itsminagerges) on Feb 1, 2017 at 2:14pm PST A photo posted by @yonslayall on Feb 1, 2017 at 2:13pm PST A photo posted by New Orleans Bounce (@neworleansbounce) on Feb 1, 2017 at 2:10pm PST ON AIR: Weekdays 2PM-6PM Angie Martinez is recognized as one of the most influential personalities in popular culture and multi-media. Originally known as \\u201cThe Voice of New York,\\u201d Angie\\u2019s nearly 20 years of on-air hosting experience has led her to become the media trailblazer she is today: multimedia host, spokeswoman, actress, recording artist, entertainment personality, author and philanthropist. Known for her signature laid back style of interviewing, Angie is well respected \\u2014 and praised \\u2014 for her unique ability to get up-close and personal with larger-than-life personalities. Her storied experience and poise on the airwaves while interviewing such global icons as Bill & Hillary Clinton, Derek Jeter, Spike Lee, JAY Z and Beyonc\\u00e9 resulted in Martinez adding Special NY Correspondent for Extra TV to her current scope of work, where she has (so far) worked alongside Oprah Winfrey, Lady Gaga, Robert DeNiro and more. In addition to her professional work across the media landscape, Angie has become a passionate cultural voice for her community and fan base through HealthyLatinEating.com, which she founded in 2014 to share her love for \\u2014 and support of \\u2014 healthy Latin cuisine. And most recently, Angie dedicated herself to training for and completing the 2014 TCS New York City Marathon in support of The PitCCh In Foundation, where her team raised over $200,000 to benefit inner city youth. Fans can tune into The Angie Martinez Show on New York\\u2019s Power 105.1, weekdays from 2pm - 6pm, and on Miami\\u2019s The Beat 103.5, middays from 10am\\u20132pm. Fans can also access The Angie Martinez Show through iHeartMedia.com and the iHeartMedia app. Contact \",\n",
      "    \"url\": \"http://power1051.iheart.com/onair/angie-martinez-54909/here-are-the-funniest-reaction-memes-15527130/\",\n",
      "    \"site\": \"iheart.com\",\n",
      "    \"domain_rank\": 1524,\n",
      "    \"engagement_scores\": {\n",
      "        \"log_weigh\": 287,\n",
      "        \"log_no_weigh\": 287,\n",
      "        \"no_log_weigh\": 437388,\n",
      "        \"no_log_no_weight\": 437388,\n",
      "        \"original\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(data_all_ordered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently using only titles to save on training time\n",
    "all_text = pp.get_all_text(titles)\n",
    "words = pp.get_words(all_text)\n",
    "\n",
    "# score: 'original', 'log_weigh', 'log_no_weigh', 'no_log_weigh', 'no_log_no_weigh'\n",
    "scores = pp.get_scores(data_all_ordered)\n",
    "mean, std = pp.get_mean_std(scores)\n",
    "scores = pp.scale_data(scores, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pp.tokenize_words(words)\n",
    "update_dict['embed_in'] = len(tokens)\n",
    "title_tokens = pp.tokenize_titles(titles, tokens)\n",
    "title_lengths = pp.get_title_lengths(title_tokens)\n",
    "title_tokens, titles, scores = pp.remove_shorts(title_tokens, titles, scores, min_len=3)\n",
    "padded_titles = pp.pad_titles(title_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (786, 25) \n",
      "Validation Size: (98, 25) \n",
      "Test Size: (99, 25)\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, test_x = pp.split_data(padded_titles)\n",
    "train_y, val_y, test_y = pp.split_data(scores)\n",
    "\n",
    "print(\"Train Size: {}\".format(train_x.shape),\n",
    "      \"\\nValidation Size: {}\".format(val_x.shape),\n",
    "      \"\\nTest Size: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pp.create_tensor_dataset(train_x, train_y)\n",
    "val_data = pp.create_tensor_dataset(val_x, val_y)\n",
    "test_data = pp.create_tensor_dataset(test_x, test_y)\n",
    "\n",
    "train_loader = pp.create_loader(train_data)\n",
    "val_loader = pp.create_loader(val_data)\n",
    "test_loader = pp.create_loader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_BiLSTM(\n",
      "  (embed): Embedding(4317, 200, padding_idx=0)\n",
      "  (bilstm): LSTM(200, 200, num_layers=2, dropout=0.1, bidirectional=True)\n",
      "  (fc1): Linear(in_features=700, out_features=350, bias=True)\n",
      "  (fc2): Linear(in_features=350, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "architecture = ml.MODELS[ml_choice]\n",
    "hps = hp.setup_hparams(architecture, update_dict)\n",
    "hps = pp.update_hps(hps)\n",
    "model = ml.CNN_BiLSTM(hps)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of train failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Code\\Jupyter Notebook\\news-engagement-prediction\\news-engagement-prediction\\train.py\", line 4, in <module>\n",
      "    from termcolor import colored\n",
      "ModuleNotFoundError: No module named 'termcolor'\n",
      "]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hps_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f6e392be256b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mml_file_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Code\\Jupyter Notebook\\news-engagement-prediction\\news-engagement-prediction\\train.py\u001b[0m in \u001b[0;36mcreate_folder\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhps_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhps_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hps_data' is not defined"
     ]
    }
   ],
   "source": [
    "folder = tr.create_folder()\n",
    "seed = tr.set_seed()\n",
    "model, hps, ml_file_losses = tr.run_all(train_loader, test_loader, folder, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Eugene\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...\n",
      "Train Loss: 8.227258\n",
      "Epoch: 10/100...\n",
      "Train Loss: 8.051239\n",
      "Epoch: 20/100...\n",
      "Train Loss: 7.995482\n",
      "Epoch: 30/100...\n",
      "Train Loss: 7.974991\n",
      "Epoch: 40/100...\n",
      "Train Loss: 7.969732\n",
      "Epoch: 50/100...\n",
      "Train Loss: 8.077700\n",
      "Epoch: 60/100...\n",
      "Train Loss: 8.028047\n"
     ]
    }
   ],
   "source": [
    "train_losses = tr.run(model, train_loader, opt, criterion, hps)\n",
    "gp.graph_losses(train_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
