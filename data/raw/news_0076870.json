{"organizations": [], "uuid": "0927f370752b717665b2e723f455fe19c1744fe2", "thread": {"social": {"gplus": {"shares": 1}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 3}, "facebook": {"likes": 238, "shares": 238, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.cbc.ca", "main_image": "https://i.cbc.ca/1.3974587.1486670434!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_1180/social-media.jpg", "site_section": "", "section_title": "", "url": "http://www.cbc.ca/news/opinion/online-toxicity-1.4001767", "country": "US", "domain_rank": 1123, "title": "Online toxicity might just be an issue of bad design - CBC News | Opinion", "performance_score": 2, "site": "cbc.ca", "participants_count": 0, "title_full": "Online toxicity might just be an issue of bad design - CBC News | Opinion", "spam_score": 0.129, "site_type": "news", "published": "2017-02-28T07:00:00.000+02:00", "replies_count": 0, "uuid": "0927f370752b717665b2e723f455fe19c1744fe2"}, "author": "", "url": "http://www.cbc.ca/news/opinion/online-toxicity-1.4001767", "ord_in_thread": 0, "title": "Online toxicity might just be an issue of bad design - CBC News | Opinion", "locations": [], "entities": {"persons": [{"name": "jigsaw", "sentiment": "none"}, {"name": "jeffrey lin", "sentiment": "none"}, {"name": "justin", "sentiment": "none"}, {"name": "ramona pringle", "sentiment": "none"}], "locations": [{"name": "jigsaw", "sentiment": "none"}], "organizations": [{"name": "cbc news | opinion opinion online", "sentiment": "negative"}, {"name": "rta school of media", "sentiment": "none"}, {"name": "npr", "sentiment": "none"}, {"name": "toronto star", "sentiment": "none"}, {"name": "ryerson university", "sentiment": "none"}, {"name": "reuters", "sentiment": "none"}, {"name": "tribunal", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "wikipedia", "sentiment": "none"}, {"name": "cbc", "sentiment": "none"}, {"name": "transmedia zone", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Opinion Online toxicity might just be an issue of bad design Anonymity sets the foundation for aggression, and the lack of consequences is what keeps the harassment going By Ramona Pringle, for CBC News Posted: Feb 28, 2017 5:00 AM ET Last Updated: Feb 28, 2017 5:00 AM ET Companies such as Google and Riot Games are implementing new strategies to tackle poisonous speech. (Justin Sullivan/Getty Images) About The Author \nRamona Pringle Technology Columnist \nRamona Pringle is an assistant professor in the RTA School of Media and director of the Transmedia Zone at Ryerson University. She is a CBC contributor who writes and reports on the relationship between people and technology. Related Stories In a bad mood and on the internet? You could become a troll \nThe misanthrope's view of the internet is that it's a hotbed for hate speech and angry trolling, and that it will forever be so. \nThat's because, according to this view, the online world is basically lawless frontier where we're free from the structure, confines and civility of real life. Another perspective is that the internet simply acts as a massive floodlight, exposing the ugliest parts of human nature. \nBut new approaches to taming trolls show that the current state of online toxicity may just be an issue of bad design. Companies such as Google and Riot Games — the makers of the massive multiplayer game League of Legends — are implementing new strategies to tackle poisonous speech, and these solutions might also prove successful in taming trolls on news sites and other online communities. The old tactics \nThe existing strategy for dealing with toxic speech has, by and large, been to raise a white flag: news outlets such as the Toronto Star , NPR, Reuters, Popular Science, The Telegraph and Recode have all closed down their comment sections. Others have eliminated the anonymity element, believing that by forcing commenters to use their real names, it will inspire more accountability. \nThese approaches are flawed, however. The internet is all about engaging with your audience, something that becomes much more difficult when comments are prohibited. What's more, anonymity is one of the most powerful assets of the internet: it allows individuals to explore different aspects of their identities and express their beliefs without fear. We should be able to rein in the bad without having to forfeit the good. \nBeyond that, a number of studies show that anonymity might not be driving online toxicity after all. Rather, it could very well be the lack of repercussions and real-life consequences — coupled with anonymity — that fuel nasty behaviour online. Indeed, anonymity might set the foundation for aggression, but the lack of consequences is arguably what keeps the harassment going. The new tools \nJeffrey Lin, a designer at Riot Games, is trying to remedy that with a tool called \"The Tribunal.\" Using The Tribunal, League of Legends players report behaviour they find unacceptable, and other community members then vote on whether they believe the behaviours permissible or not. After initially implementing the tool, Riot Games incorporated artificial intelligence to make the whole process more efficient. Humans still identify which behaviour is and isn't acceptable, but the machine learning system delivers swift, customized consequences and penalties, such as chat restrictions and temporary bans from the game. \nAdvice from a 10-year-old on dealing with online trolls \nNow Google is trying a similar approach. The company's tech incubator, Jigsaw, along with its C ounter Abuse Technology team, recently launched Perspective, a public API that uses artificial intelligence to automatically flag toxic online speech. By comparing new comments with a large data set comprised of archived comments, previously flagged as toxic, from sources such as Wikipedia or online news comment sections, Jigsaw believes it can positively identify hateful speech. As a result, a user's commenting privileges may be revoked, or else, he or she might be subject to \"shadowbanning,\" whereby comments appear invisible to other members of the community. \nBoth of these models suggest that maybe the current of online toxicity isn't inevitable, or irreversible. Instead, perhaps it is just an issue of bad design, or as is often the case, of no design at all. When people step out of line, there need to be consequences, which is where good design strategies and machine learning can help. Online discourse can get better; communities just need the tools to help make it happen. \nThis column is part of CBC's Opinion section. For more information about this section, please read this editor's blog and our FAQ . Report Typo or Error Send Feedback \nTo encourage thoughtful and respectful conversations, first and last names will appear with each submission to CBC/Radio-Canada's online communities (except in children and youth-oriented communities). Pseudonyms will no longer be permitted. \nBy submitting a comment, you accept that CBC has the right to reproduce and publish that comment in whole or in part, in any manner CBC chooses. Please note that CBC does not endorse the opinions expressed in comments. Comments on this story are moderated according to our Submission Guidelines . Comments are welcome while open. We reserve the right to close comments at any time. Submission Policy \nNote: The CBC does not necessarily endorse any of the views posted. By submitting your comments, you acknowledge that CBC has the right to reproduce, broadcast and publicize those comments or any part thereof in any manner whatsoever. Please note that comments are moderated and published according to our submission guidelines . Latest News ", "external_links": [], "published": "2017-02-28T07:00:00.000+02:00", "crawled": "2017-02-28T12:49:28.127+02:00", "highlightTitle": ""}