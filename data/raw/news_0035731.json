{"organizations": [], "uuid": "e9eb9a192b59839a1b935461f9c07d63e6f95fff", "thread": {"social": {"gplus": {"shares": 30}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 85}, "facebook": {"likes": 856, "shares": 856, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "medium.com", "main_image": "https://cdn-images-1.medium.com/max/1200/1*xs89ZrWSDXc4lh7y9QYsKw.png", "site_section": "", "section_title": "", "url": "https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46", "country": "US", "domain_rank": 380, "title": "DeepMind just published a mind blowing paper: PathNet.", "performance_score": 8, "site": "medium.com", "participants_count": 0, "title_full": "DeepMind just published a mind blowing paper: PathNet.", "spam_score": 0.118, "site_type": "news", "published": "2017-02-18T16:47:00.000+02:00", "replies_count": 0, "uuid": "e9eb9a192b59839a1b935461f9c07d63e6f95fff"}, "author": "Th O Szymkowiak", "url": "https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46", "ord_in_thread": 0, "title": "DeepMind just published a mind blowing paper: PathNet.", "locations": [], "entities": {"persons": [{"name": "deepmind", "sentiment": "neutral"}, {"name": "pong", "sentiment": "none"}], "locations": [], "organizations": [{"name": "pathnet", "sentiment": "neutral"}, {"name": "mcgill", "sentiment": "neutral"}, {"name": "mcgill artificial intelligence s", "sentiment": "neutral"}, {"name": "general artificial intelligence", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Undergraduate student at McGill and AI Enthusiast. President of the McGill Artificial Intelligence Society. Feb 15 DeepMind just published a mind blowing paper: PathNet. Potentially describing how general artificial intelligence will look like. Since scientists started building and training neural networks, there has always been one barrier called transfer learning. Transfer learning is the capability of an AI to learn from different tasks and apply its pre-learned knowledge to a totally new task. It is implicit that with this precedent knowledge, the AI will perform better and train faster than de novo neural networks on the new task. DeepMind is maybe on the path of solving this with PathNet. PathNet is a network of neural networks , trained using both stochastic gradient descent and a genetic selection method. PathNet is composed of layers of modules. Each module is a Neural Network of any type, it could be convolutional, recurrent, feedforward and whatnot. Each of those nine boxes is the PathNet at a different iteration. In this case, the PathNet was trained on two different games using a Advantage Actor-critic or A3C. Although Pong and Alien seem very different at first, we observe a positive transfer learning using PathNet (take a look at the score graph). How does it train First of all, we need to define the modules. Let L be the number of layers and N be the maximum number of modules per layer (the paper indicates that N is typically 3 or 4). The last layer is dense and not shared between the different tasks. Using A3c, this last layer represent the value function and policy evaluation. After defining those modules, P genotypes (=pathways) are generated in the network. Due to the asynchronous nature of A3c, multiple workers are spawned to evaluate each genotype. After T episodes, a worker selects a couple of other pathways to compare to, if any of those pathways have a better fitness, it adopts it and continues training with that new pathway. If not, the worker continues evaluating the fitness of its pathway. Pathway are trained using stochastic gradient descent with back-propagation throughout a single path at a time. This guarantees a manageable training time. Transfer learning After learning a task, the network fixes all parameters on the optimal path. All other parameters are reinitialized because otherwise, according to the paper, PathNet performs poorly on new tasks. Using A3C, the optimal path of the previous task is not modified by the back-propagation pass on the PathNet for a new task. This can be view as a safety guard to not erase previous knowledge Results PathNet does not work on every pair of game (blue cells equal negative transfer). But the important takeaway here is that PathNet actually works for some couple of games, which is already an immense step towards better transfer learning. Extrapolating We can imagine that in the future, we will have giant AIs trained on thousands of tasks and able to generalize. In short, General Artificial Intelligence. Link to the paper: https://arxiv.org/pdf/1701.08734.pdf Bonus — Training of PathNet in video ", "external_links": [], "published": "2017-02-18T16:47:00.000+02:00", "crawled": "2017-02-18T11:47:45.458+02:00", "highlightTitle": ""}